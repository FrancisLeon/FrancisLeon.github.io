<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta name="description" content="去人迹罕至的地方，留下自己的足迹。">
<meta name="keywords" content="reinforcement learning, deep learning, machine learning">
<meta property="og:type" content="website">
<meta property="og:title" content="Truly">
<meta property="og:url" content="http://yoursite.com/page/9/index.html">
<meta property="og:site_name" content="Truly">
<meta property="og:description" content="去人迹罕至的地方，留下自己的足迹。">
<meta property="og:locale" content="default">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Truly">
<meta name="twitter:description" content="去人迹罕至的地方，留下自己的足迹。">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/page/9/"/>





  <title>Truly</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Truly</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/24/multi-level-GAN-code解读/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chu Lin">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Truly">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/06/24/multi-level-GAN-code解读/" itemprop="url">multi-level-GAN-code解读</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-06-24T11:24:59-07:00">
                2018-06-24
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/transfer-learning/" itemprop="url" rel="index">
                    <span itemprop="name">transfer learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>接下来我开始解读，<a href="https://arxiv.org/abs/1802.10349" target="_blank" rel="noopener">这篇</a>论文的<a href="https://github.com/wasidennis/AdaptSegNet" target="_blank" rel="noopener">code</a></p>
<h1 id="Network-Structure-and-Training"><a href="#Network-Structure-and-Training" class="headerlink" title="Network Structure and Training"></a>Network Structure and Training</h1><h2 id="Discriminator"><a href="#Discriminator" class="headerlink" title="Discriminator"></a>Discriminator</h2><p>For the discriminator, we use an architecture similar to but utilize all fully-convolutional lay- ers to <strong>retain the spatial information</strong>.<br>总是来说就是，5层卷积网络，kernel是4 x 4 stride是2，channel是{64, 128, 256, 512, 1}。<br>除了最后一层卷积层，其他所有层都是用参数为0.2的leaky ReLU。在最后一层卷积层之后加了一个up-sampling的层，使得最后一层和输入图片的大小是一样的。他们没有使用batch-normalization层，因为他们用小的batch size一起训练判别器和分割网络。(?)</p>
<ul>
<li>batch normalization:</li>
</ul>
<h2 id="Segmentation-Network"><a href="#Segmentation-Network" class="headerlink" title="Segmentation Network"></a>Segmentation Network</h2><p>他们用DeepLab-v2 和 ResNet-101来作为他们分割的baseline，由于memory的问题，他们没有使用multi-scale。<br>他们去掉了最后的分类的一层，然后将最后的两层卷积stride从2改成1。这使得输出的feature maps是输入图片大小的1/8。为了使这个更大，他们在conv4和conv5用了stride分别是2，4的dilated conv。这后面又用了Atrous Spatial Pyramid Pooling (ASPP)作为最后的分类器。在ASPP后面，他们也采用了输出的是softmax的up-sampling层，这层输出的大小和输入的图片大小也是一样的。</p>
<ul>
<li>ASPP:</li>
</ul>
<h2 id="Multi-level-Adaptation-Model"><a href="#Multi-level-Adaptation-Model" class="headerlink" title="Multi-level Adaptation Model"></a>Multi-level Adaptation Model</h2><p>上面的构成了他们single-level的网络结构。为了构建multi-level的结构，他们将conv4的feature map和一个作为辅助分类器的ASPP模块相结合。和single-level类似，这里面也加了一个同样结构的判别器来进行对抗学习。如图：<br><img src="/images/multi-GAN-structure.jpeg" width="70%" height="70%"></p>
<h2 id="Train"><a href="#Train" class="headerlink" title="Train"></a>Train</h2><p>作者发现，将segmentation network和discriminitor一起训练效率会比较高。<br>对源域将图片$I_s$向前传最后得到$P_s$，以及优化$L_{seg}$。对于目标域，我们将得到的$P_t$和$P_s$一起输入到判别器里面，然后优化$L_{d}$。此外，对于$P_t$，我们还需要计算对抗损失$L_{ad}$。</p>
<h2 id="Loss-Function"><a href="#Loss-Function" class="headerlink" title="Loss Function"></a>Loss Function</h2><ul>
<li><p>whole objective:<br>  $L(I_s, I_t) = L_{seg}(I_s) + \lambda L_{adv}(I_t)$</p>
<ul>
<li>$L_{seg}(I_s)$<br>  cross-entropy loss using ground truth annotations in the source domain</li>
<li>$L_{adv}$<br>  对抗损失，用来使得源域的预期的数据分布和目标域相近</li>
<li>$\lambda_{abv}$<br>  这个weight用来平衡这两个loss</li>
</ul>
</li>
<li><p>discriminitor:</p>
<ul>
<li><p>segmentation softmax output:<br>  $P = G(I) \in R^{HxWxC}$, 这里C是种类数，这里C是19</p>
</li>
<li><p>cross-entropy loss：<br>  我们将P传到全卷积的判别器D里面：$L_d(P) = - \sum_{h, w}((1 - z)log(D(P)^{(h,w,0)})) + zlog(D(P)^{(h,w,1)})$，这个是binary cross entropy，这里z = 0，表示来自target，z = 1表示来自source</p>
</li>
</ul>
</li>
<li><p>segmentation network:</p>
<ul>
<li>segmentation loss:<br>  在源域的话我们正常训练，还是由cross-entropy loss来定义：$L_{seg}(I_s) = -\sum_{h, w}\sum_{c \in C}Y_s^{h,w,c}log(P_s^{(h,w,c)})$</li>
<li>adversarial loss：<br>  在目标域，我们的对抗损失是：$L_{adv}(I_t) = -\sum_{h,w}log(D(G(I_t)))^{(h,w,1)}$，这个损失是用来欺骗判别器的，使得两者的预期的概率的一致</li>
</ul>
</li>
<li><p>multi-level:</p>
<ul>
<li>multi-level loss<br>  就是在low-level的feature space里面加上上面的loss，也不是很难理解：<br>  $L_{I_s, I_t} = \sum_i \lambda_i^{seg}L^i_{seg}(I_s) + \sum_i \lambda^i_{adv}L_{adv}^i(I_t)$，i表示第几层网络。</li>
</ul>
</li>
</ul>
<h1 id="Network-Code"><a href="#Network-Code" class="headerlink" title="Network Code"></a>Network Code</h1><h2 id="Discriminitor"><a href="#Discriminitor" class="headerlink" title="Discriminitor"></a>Discriminitor</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">import torch.nn as nn</span><br><span class="line">import torch.nn.functional as F</span><br><span class="line">class FCDiscriminator(nn.Module):</span><br><span class="line">	def __init__(self, num_classes, ndf = 64):</span><br><span class="line">		super(FCDiscriminator, self).__init__()</span><br><span class="line"></span><br><span class="line">		self.conv1 = nn.Conv2d(num_classes, ndf, kernel_size=4, stride=2, padding=1)</span><br><span class="line">		self.conv2 = nn.Conv2d(ndf, ndf*2, kernel_size=4, stride=2, padding=1)</span><br><span class="line">		self.conv3 = nn.Conv2d(ndf*2, ndf*4, kernel_size=4, stride=2, padding=1)</span><br><span class="line">		self.conv4 = nn.Conv2d(ndf*4, ndf*8, kernel_size=4, stride=2, padding=1)</span><br><span class="line">		self.classifier = nn.Conv2d(ndf*8, 1, kernel_size=4, stride=2, padding=1)</span><br><span class="line"></span><br><span class="line">		self.leaky_relu = nn.LeakyReLU(negative_slope=0.2, inplace=True)</span><br><span class="line">		#self.up_sample = nn.Upsample(scale_factor=32, mode=&apos;bilinear&apos;)</span><br><span class="line">		#self.sigmoid = nn.Sigmoid()</span><br><span class="line"></span><br><span class="line">	def forward(self, x):</span><br><span class="line">		x = self.conv1(x)</span><br><span class="line">		x = self.leaky_relu(x)</span><br><span class="line">		x = self.conv2(x)</span><br><span class="line">		x = self.leaky_relu(x)</span><br><span class="line">		x = self.conv3(x)</span><br><span class="line">		x = self.leaky_relu(x)</span><br><span class="line">		x = self.conv4(x)</span><br><span class="line">		x = self.leaky_relu(x)</span><br><span class="line">		x = self.classifier(x)</span><br><span class="line">		#x = self.up_sample(x)</span><br><span class="line">		#x = self.sigmoid(x) </span><br><span class="line"></span><br><span class="line">		return x</span><br></pre></td></tr></table></figure>
<p>有了上面的描述，判别器的网络还是很清楚的。</p>
<h2 id="Segmentation-Network-1"><a href="#Segmentation-Network-1" class="headerlink" title="Segmentation Network"></a>Segmentation Network</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">class ResNetMulti(nn.Module):</span><br><span class="line">    def __init__(self, block, layers, num_classes):</span><br><span class="line">        self.inplanes = 64</span><br><span class="line">        super(ResNetMulti, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,</span><br><span class="line">                               bias=False)</span><br><span class="line">        self.bn1 = nn.BatchNorm2d(64, affine=affine_par)</span><br><span class="line">        for i in self.bn1.parameters():</span><br><span class="line">            i.requires_grad = False</span><br><span class="line">        self.relu = nn.ReLU(inplace=True)</span><br><span class="line">        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1, ceil_mode=True)  # change</span><br><span class="line">        self.layer1 = self._make_layer(block, 64, layers[0])</span><br><span class="line">        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)</span><br><span class="line">        self.layer3 = self._make_layer(block, 256, layers[2], stride=1, dilation=2)</span><br><span class="line">        self.layer4 = self._make_layer(block, 512, layers[3], stride=1, dilation=4)</span><br><span class="line">        self.layer5 = self._make_pred_layer(Classifier_Module, 1024, [6, 12, 18, 24], [6, 12, 18, 24], num_classes)</span><br><span class="line">        self.layer6 = self._make_pred_layer(Classifier_Module, 2048, [6, 12, 18, 24], [6, 12, 18, 24], num_classes)</span><br></pre></td></tr></table></figure>
<p>前面应该是对resnet的结构的继承吧，后面的layer5，和layer6应该就是前面说的ASPP的classifier了，这两个分别之后参与adaptation module的部分。</p>
<h1 id="Train-Code"><a href="#Train-Code" class="headerlink" title="Train Code"></a>Train Code</h1><h2 id="Train-G"><a href="#Train-G" class="headerlink" title="Train G"></a>Train G</h2><p>类似train 原本的GAN，这里train的G其实就是segmentation network</p>
<h3 id="Train-with-source"><a href="#Train-with-source" class="headerlink" title="Train with source"></a>Train with source</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">_, batch = trainloader_iter.next()</span><br><span class="line">images, labels, _, _ = batch</span><br><span class="line">images = Variable(images).cuda(args.gpu)</span><br><span class="line"></span><br><span class="line">pred1, pred2 = model(images)</span><br><span class="line">pred1 = interp(pred1)</span><br><span class="line">pred2 = interp(pred2)</span><br><span class="line"></span><br><span class="line">loss_seg1 = loss_calc(pred1, labels, args.gpu)</span><br><span class="line">loss_seg2 = loss_calc(pred2, labels, args.gpu)</span><br><span class="line">loss = loss_seg2 + args.lambda_seg * loss_seg1</span><br><span class="line"></span><br><span class="line"># proper normalization</span><br><span class="line">loss = loss / args.iter_size</span><br><span class="line">loss.backward()</span><br><span class="line">loss_seg_value1 += loss_seg1.data.cpu().numpy()[0] / args.iter_size</span><br><span class="line">loss_seg_value2 += loss_seg2.data.cpu().numpy()[0] / args.iter_size</span><br></pre></td></tr></table></figure>
<p>train with source这里的loss就是：$\sum_i \lambda_i^{seg}L^i_{seg}(I_s)$，$L_{seg}(I_s) = -\sum_{h, w}\sum_{c \in C}Y_s^{h,w,c}log(P_s^{(h,w,c)})$</p>
<h3 id="Train-with-target"><a href="#Train-with-target" class="headerlink" title="Train with target"></a>Train with target</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">_, batch = targetloader_iter.next()</span><br><span class="line">images, _, _ = batch</span><br><span class="line">images = Variable(images).cuda(args.gpu)</span><br><span class="line"></span><br><span class="line">pred_target1, pred_target2 = model(images)</span><br><span class="line">pred_target1 = interp_target(pred_target1)</span><br><span class="line">pred_target2 = interp_target(pred_target2)</span><br><span class="line"></span><br><span class="line">D_out1 = model_D1(F.softmax(pred_target1))</span><br><span class="line">D_out2 = model_D2(F.softmax(pred_target2))</span><br><span class="line"></span><br><span class="line">loss_adv_target1 = bce_loss(D_out1,</span><br><span class="line">                            Variable(torch.FloatTensor(D_out1.data.size()).fill_(source_label)).cuda(</span><br><span class="line">                                args.gpu))</span><br><span class="line"></span><br><span class="line">loss_adv_target2 = bce_loss(D_out2,</span><br><span class="line">                            Variable(torch.FloatTensor(D_out2.data.size()).fill_(source_label)).cuda(</span><br><span class="line">                                args.gpu))</span><br><span class="line"></span><br><span class="line">loss = args.lambda_adv_target1 * loss_adv_target1 + args.lambda_adv_target2 * loss_adv_target2</span><br><span class="line">loss = loss / args.iter_size</span><br><span class="line">loss.backward()</span><br><span class="line">loss_adv_target_value1 += loss_adv_target1.data.cpu().numpy()[0] / args.iter_size</span><br><span class="line">loss_adv_target_value2 += loss_adv_target2.data.cpu().numpy()[0] / args.iter_size</span><br></pre></td></tr></table></figure>
<p>train target对应的就是：$\sum_i \lambda^i_{adv}L_{adv}^i(I_t)$，$L_{adv}(I_t) = -\sum_{h,w}log(D(G(I_t)))^{(h,w,1)}$</p>
<h2 id="Train-D"><a href="#Train-D" class="headerlink" title="Train D"></a>Train D</h2><h3 id="Train-with-source-1"><a href="#Train-with-source-1" class="headerlink" title="Train with source"></a>Train with source</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"># labels for adversarial training</span><br><span class="line">source_label = 0</span><br><span class="line"></span><br><span class="line">pred1 = pred1.detach()</span><br><span class="line">pred2 = pred2.detach()</span><br><span class="line"></span><br><span class="line">D_out1 = model_D1(F.softmax(pred1))</span><br><span class="line">D_out2 = model_D2(F.softmax(pred2))</span><br><span class="line"></span><br><span class="line">loss_D1 = bce_loss(D_out1,</span><br><span class="line">                    Variable(torch.FloatTensor(D_out1.data.size()).fill_(source_label)).cuda(args.gpu))</span><br><span class="line"></span><br><span class="line">loss_D2 = bce_loss(D_out2,</span><br><span class="line">                    Variable(torch.FloatTensor(D_out2.data.size()).fill_(source_label)).cuda(args.gpu))</span><br><span class="line"></span><br><span class="line">loss_D1 = loss_D1 / args.iter_size / 2</span><br><span class="line">loss_D2 = loss_D2 / args.iter_size / 2</span><br><span class="line"></span><br><span class="line">loss_D1.backward()</span><br><span class="line">loss_D2.backward()</span><br><span class="line"></span><br><span class="line">loss_D_value1 += loss_D1.data.cpu().numpy()</span><br><span class="line">loss_D_value2 += loss_D2.data.cpu().numpy()</span><br></pre></td></tr></table></figure>
<p>$L_d(P) = - \sum_{h, w}((1 - z)log(D(P)^{(h,w,0)})) + zlog(D(P)^{(h,w,1)})$，z = 0</p>
<h3 id="Train-with-target-1"><a href="#Train-with-target-1" class="headerlink" title="Train with target"></a>Train with target</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"># labels for adversarial training</span><br><span class="line">target_label = 1</span><br><span class="line"></span><br><span class="line">pred_target1 = pred_target1.detach()</span><br><span class="line">pred_target2 = pred_target2.detach()</span><br><span class="line"></span><br><span class="line">D_out1 = model_D1(F.softmax(pred_target1))</span><br><span class="line">D_out2 = model_D2(F.softmax(pred_target2))</span><br><span class="line"></span><br><span class="line">loss_D1 = bce_loss(D_out1,</span><br><span class="line">                    Variable(torch.FloatTensor(D_out1.data.size()).fill_(target_label)).cuda(args.gpu))</span><br><span class="line"></span><br><span class="line">loss_D2 = bce_loss(D_out2,</span><br><span class="line">                    Variable(torch.FloatTensor(D_out2.data.size()).fill_(target_label)).cuda(args.gpu))</span><br><span class="line"></span><br><span class="line">loss_D1 = loss_D1 / args.iter_size / 2</span><br><span class="line">loss_D2 = loss_D2 / args.iter_size / 2</span><br><span class="line"></span><br><span class="line">loss_D1.backward()</span><br><span class="line">loss_D2.backward()</span><br><span class="line"></span><br><span class="line">loss_D_value1 += loss_D1.data.cpu().numpy()</span><br><span class="line">loss_D_value2 += loss_D2.data.cpu().numpy()</span><br></pre></td></tr></table></figure>
<p>$L_d(P) = - \sum_{h, w}((1 - z)log(D(P)^{(h,w,0)})) + zlog(D(P)^{(h,w,1)})$，z = 1</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/20/leetcode体系/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chu Lin">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Truly">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/06/20/leetcode体系/" itemprop="url">leetcode体系</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-06-20T22:39:09-07:00">
                2018-06-20
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/leetcode/" itemprop="url" rel="index">
                    <span itemprop="name">leetcode</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>leetcode里面考察很多的算法知识：</p>
<ol>
<li>基本的数据结构：<br> queue, stack, heap, tree, graph，linked-list，trie</li>
<li>一些基本的算法：<br> sort<br> binary search</li>
<li>一些高级点的算法：<br> dp，bfs，dfs</li>
<li>别忘了位运算，和‘2’有关的时候很有用，可以简化问题<br>边刷，边整理和总结</li>
</ol>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/20/GAN-pytorch/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chu Lin">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Truly">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/06/20/GAN-pytorch/" itemprop="url">GAN@pytorch</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-06-20T15:45:51-07:00">
                2018-06-20
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/transfer-learning/" itemprop="url" rel="index">
                    <span itemprop="name">transfer learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>莫烦python也给了pytorch的GAN<a href="https://morvanzhou.github.io/tutorials/machine-learning/torch/4-06-GAN/" target="_blank" rel="noopener">版本</a>：<br>然后莫烦的全部代码在<a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/406_GAN.py" target="_blank" rel="noopener">这里</a></p>
<h1 id="hyper-parameter"><a href="#hyper-parameter" class="headerlink" title="hyper parameter"></a>hyper parameter</h1><p>新手画家 (Generator) 在作画的时候需要有一些灵感 (random noise), 我们这些灵感的个数定义为 N_IDEAS. 而一幅画需要有一些规格, 我们将这幅画的画笔数定义一下, N_COMPONENTS 就是一条一元二次曲线(这幅画画)上的点个数. 为了进行批训练, 我们将一整批话的点都规定一下(PAINT_POINTS).<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import torch.nn as nn</span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line">torch.manual_seed(1) # reproducible</span><br><span class="line">np.random.seed(1)</span><br><span class="line"></span><br><span class="line"># hyper parameter</span><br><span class="line">BATCH_SIZE = 64</span><br><span class="line">LR_G = 0.0001 </span><br><span class="line">LR_D = 0.0001</span><br><span class="line">N_IDEAS = 5</span><br><span class="line">ART_COMPONENTS = 15</span><br><span class="line">PAINT_POINTS = np.vstack([np.linspace(-1, 1, ART_COMPONENTS) for _ in range(BATCH_SIZE)])</span><br></pre></td></tr></table></figure></p>
<h1 id="著名画家的画"><a href="#著名画家的画" class="headerlink" title="著名画家的画"></a>著名画家的画</h1><p>我们需要有很多画是来自著名画家的(real data), 将这些著名画家的画, 和新手画家的画都传给新手鉴赏家, 让鉴赏家来区分哪些是著名画家, 哪些是新手画家的画. 如何区分我们在后面呈现. 这里我们生成一些著名画家的画 (batch 条不同的一元二次方程曲线).</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">def artist_works(): # real target</span><br><span class="line">    a = np.random.uniform(1, 2, size = BATCH_SIZE)[:, np.newaxis]</span><br><span class="line">    paintings = a * np.power(PAINT_POINTS, 2) + (a - 1)</span><br><span class="line">    paintings = torch.from_numpy(paintings).float()</span><br><span class="line">    return paintings</span><br></pre></td></tr></table></figure>
<h1 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h1><p>这里会创建两个神经网络, 分别是 Generator (新手画家), Discriminator(新手鉴赏家). G 会拿着自己的一些灵感当做输入, 输出一元二次曲线上的点 (G 的画).</p>
<p>D 会接收一幅画作 (一元二次曲线), 输出这幅画作到底是不是著名画家的画(是著名画家的画的概率).</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">G = nn.Sequential(                      # Generator</span><br><span class="line">    nn.Linear(N_IDEAS, 128),            # random ideas (could from normal distribution)</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Linear(128, ART_COMPONENTS),     # making a painting from these random ideas</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">D = nn.Sequential(                      # Discriminator</span><br><span class="line">    nn.Linear(ART_COMPONENTS, 128),     # receive art work either from the famous artist or a newbie like G</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Linear(128, 1),</span><br><span class="line">    nn.Sigmoid(),                       # tell the probability that the art work is made by artist</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h1 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h1><p>接着我们来同时训练 D 和 G. 训练之前, 我们来看看G作画的原理. G 首先会有些灵感, G_ideas 就会拿到这些随机灵感 (可以是正态分布的随机数), 然后 G 会根据这些灵感画画. 接着我们拿着著名画家的画和 G 的画, 让 D 来判定这两批画作是著名画家画的概率.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">for step in range(10000):</span><br><span class="line">    artist_paintings = artist_works()           # real painting from artist</span><br><span class="line">    G_ideas = torch.randn(BATCH_SIZE, N_IDEAS)    # random ideas</span><br><span class="line">    G_paintings = G(G_ideas())                  # fake painting from G (random ideas)</span><br><span class="line"></span><br><span class="line">    prob_artist0 = D(artist_paintings)          # D try to increase this prob</span><br><span class="line">    prob_artist1 = D(G_paintings)               # D try to reduce this prob</span><br></pre></td></tr></table></figure>
<p>然后计算有多少来之画家的画猜对了, 有多少来自 G 的画猜对了, 我们想最大化这些猜对的次数. 这也就是 log(D(x)) + log(1-D(G(z)) 在论文中的形式. 而因为 torch 中提升参数的形式是最小化误差, 那我们把最大化 score 转换成最小化 loss, 在两个 score 的合的地方加一个符号就好. 而 G 的提升就是要减小 D 猜测 G 生成数据的正确率, 也就是减小 D_score1.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">D_loss = -torch.mean(torch.log(prob_artist0) + torch.log(1. - prob_artist1))</span><br><span class="line">G_loss = torch.mean(torch.log(1. - prob_artist1))</span><br></pre></td></tr></table></figure>
<p>最后我们在根据 loss 提升神经网络就好了.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">opt_D.zero_grad()</span><br><span class="line">D_loss.backward(retain_graph=True)              # retain_graph 这个参数是为了再次使用计算图纸</span><br><span class="line">opt_D.step()</span><br><span class="line"></span><br><span class="line">opt_G.zero_grad()</span><br><span class="line">G_loss.backward()</span><br><span class="line">opt_G.step()</span><br></pre></td></tr></table></figure></p>
<p>之后就是，开始整理总结最近在看的那篇论文作者代码的解读了。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/20/pytorch/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chu Lin">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Truly">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/06/20/pytorch/" itemprop="url">pytorch 入门</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-06-20T15:30:49-07:00">
                2018-06-20
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/tech/" itemprop="url" rel="index">
                    <span itemprop="name">tech</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>最近在看domain adaptation的代码，需要学习pytorch，然后参考<a href="https://morvanzhou.github.io/" target="_blank" rel="noopener">莫烦python的代码</a>，自己整理一遍:</p>
<h1 id="用-Numpy-还是-Torch"><a href="#用-Numpy-还是-Torch" class="headerlink" title="用 Numpy 还是 Torch"></a>用 Numpy 还是 Torch</h1><p>Torch 自称为神经网络界的 Numpy, 因为他能将 torch 产生的 tensor 放在 GPU 中加速运算 (前提是你有合适的 GPU), 就像 Numpy 会把 array 放在 CPU 中加速运算. 所以神经网络的话, 当然是用 Torch 的 tensor 形式数据最好咯. 就像 Tensorflow 当中的 tensor 一样.</p>
<p>当然, 我们对 Numpy 还是爱不释手的, 因为我们太习惯 numpy 的形式了. 不过 torch 看出来我们的喜爱, 他把 torch 做的和 numpy 能很好的兼容. 比如这样就能自由地转换 numpy array 和 torch tensor 了:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">np_data = np.arange(6).reshape((2, 3))</span><br><span class="line">torch_data = torch.from_numpy(np_data)</span><br><span class="line">tensor2array = torch_data.numpy()</span><br><span class="line">print(</span><br><span class="line">    &apos;\nnumpy array:&apos;, np_data,          # [[0 1 2], [3 4 5]]</span><br><span class="line">    &apos;\ntorch tensor:&apos;, torch_data,      # 0 1 2 \n 3 4 5 [torch.LongTensor of 2 x 3]</span><br><span class="line">    &apos;\ntensor to array&apos;: tensor2array,  # [[0 1 2], [3 4 5]]</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h1 id="Torch-中的数学运算"><a href="#Torch-中的数学运算" class="headerlink" title="Torch 中的数学运算"></a>Torch 中的数学运算</h1><h2 id="简单运算"><a href="#简单运算" class="headerlink" title="简单运算"></a>简单运算</h2><p>其实 torch 中 tensor 的运算和 numpy array 的如出一辙, 我们就以对比的形式来看. 如果想了解 torch 中其它更多有用的运算符，<a href="https://pytorch.org/docs/stable/torch.html" target="_blank" rel="noopener">API就是你要去的地方</a>.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"># abs 绝对值的计算</span><br><span class="line">data = [-1, -2, 1, 2]</span><br><span class="line">tensor = torch.FloatTensor(data)</span><br><span class="line">print(</span><br><span class="line">    &apos;\nabs&apos;,</span><br><span class="line">    &apos;\nnumpy: &apos;, np.abs(data),          # [1 2 1 2]</span><br><span class="line">    &apos;\ntorch: &apos;, torch.abs(tensor)      # [1 2 1 2]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"># sin   三角函数 sin</span><br><span class="line">print(</span><br><span class="line">    &apos;\nsin&apos;,</span><br><span class="line">    &apos;\nnumpy: &apos;, np.sin(data),      # [-0.84147098 -0.90929743  0.84147098  0.90929743]</span><br><span class="line">    &apos;\ntorch: &apos;, torch.sin(tensor)  # [-0.8415 -0.9093  0.8415  0.9093]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"># mean  均值</span><br><span class="line">print(</span><br><span class="line">    &apos;\nmean&apos;,</span><br><span class="line">    &apos;\nnumpy: &apos;, np.mean(data),         # 0.0</span><br><span class="line">    &apos;\ntorch: &apos;, torch.mean(tensor)     # 0.0</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h2 id="矩阵的运算"><a href="#矩阵的运算" class="headerlink" title="矩阵的运算"></a>矩阵的运算</h2><p>除了简单的计算, 矩阵运算才是神经网络中最重要的部分. 所以我们展示下矩阵的乘法. 注意一下包含了一个 numpy 中可行, 但是 torch 中不可行的方式.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"># matrix multiplication 矩阵点乘</span><br><span class="line">data = [[1,2], [3,4]]</span><br><span class="line">tensor = torch.FloatTensor(data)  # 转换成32位浮点 tensor</span><br><span class="line"># correct method</span><br><span class="line">print(</span><br><span class="line">    &apos;\nmatrix multiplication (matmul)&apos;,</span><br><span class="line">    &apos;\nnumpy: &apos;, np.matmul(data, data),     # [[7, 10], [15, 22]]</span><br><span class="line">    &apos;\ntorch: &apos;, torch.mm(tensor, tensor)   # [[7, 10], [15, 22]]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"># !!!!  下面是错误的方法 !!!!</span><br><span class="line">data = np.array(data)</span><br><span class="line">print(</span><br><span class="line">    &apos;\nmatrix multiplication (dot)&apos;,</span><br><span class="line">    &apos;\nnumpy: &apos;, data.dot(data),        # [[7, 10], [15, 22]] 在numpy 中可行</span><br><span class="line">    &apos;\ntorch: &apos;, tensor.dot(tensor)     # torch 会转换成 [1,2,3,4].dot([1,2,3,4) = 30.0</span><br><span class="line">)</span><br></pre></td></tr></table></figure></p>
<p>新版本中(&gt;=0.3.0), 关于 tensor.dot() 有了新的改变, 它只能针对于一维的数组. 所以上面的有所改变.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tensor.dot(tensor)     # torch 会转换成 [1,2,3,4].dot([1,2,3,4) = 30.0</span><br><span class="line"></span><br><span class="line"># 变为</span><br><span class="line">torch.dot(tensor.dot(tensor)</span><br></pre></td></tr></table></figure></p>
<h1 id="Variable"><a href="#Variable" class="headerlink" title="Variable"></a>Variable</h1><h2 id="什么是Variable"><a href="#什么是Variable" class="headerlink" title="什么是Variable"></a>什么是Variable</h2><p>这个感觉和tensorflow里面的一样，就是存有变化的数值的地方，然后这个变化的值就是tensor。然后，这里Variable可能有点像tensorflow里面的placeholder吧</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torch.autograd import Variable</span><br><span class="line"></span><br><span class="line">tensor = torch.FloatTensor(([1, 2], [3, 4]))</span><br><span class="line"># requires_grad 是参不参与误差的反向传播，要不要计算梯度</span><br><span class="line">variable = Variable(tensor, requires_grad=True)</span><br><span class="line"></span><br><span class="line">print(tensor)</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"> 1  2</span><br><span class="line"> 3  4</span><br><span class="line">[torch.FloatTensor of size 2x2]</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">print(variable)</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">Variable containing:</span><br><span class="line"> 1  2</span><br><span class="line"> 3  4</span><br><span class="line">[torch.FloatTensor of size 2x2]</span><br><span class="line">&quot;&quot;&quot;</span><br></pre></td></tr></table></figure>
<h2 id="Variable-计算，梯度"><a href="#Variable-计算，梯度" class="headerlink" title="Variable 计算，梯度"></a>Variable 计算，梯度</h2><p>我们再对比一下 tensor 的计算和 variable 的计算.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">t_out = torch.mean(tensor*tensor)       # x^2</span><br><span class="line">v_out = torch.mean(variable*variable)   # x^2</span><br><span class="line">print(t_out)</span><br><span class="line">print(v_out)    # 7.5</span><br></pre></td></tr></table></figure></p>
<p>这里我们应该也看不出Variable和一般tensor的不同，和tensorflow类似，Variable参与计算时，也是在打一个computational graph （原来是将所有的计算步骤 (节点) 都连接起来, 最后进行误差反向传递的时候, 一次性将所有 variable 里面的修改幅度 (梯度) 都计算出来, 而 tensor 就没有这个能力啦，毕竟，tensor 只是一个值而已。）</p>
<p><strong>v_out = torch.mean(variable*variable)</strong> 就是在计算图中添加的一个计算步骤, 计算误差反向传递的时候有他一份功劳, 我们就来举个例子:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">v_out.backward()    # 模拟 v_out 的误差反向传递</span><br><span class="line"></span><br><span class="line"># 下面两步看不懂没关系, 只要知道 Variable 是计算图的一部分, 可以用来传递误差就好.</span><br><span class="line"># v_out = 1/4 * sum(variable*variable) 这是计算图中的 v_out 计算步骤</span><br><span class="line"># 针对于 v_out 的梯度就是, d(v_out)/d(variable) = 1/4*2*variable = variable/2</span><br><span class="line"></span><br><span class="line">print(variable.grad)    # 初始 Variable 的梯度</span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line"> 0.5000  1.0000</span><br><span class="line"> 1.5000  2.0000</span><br><span class="line">&apos;&apos;&apos;</span><br></pre></td></tr></table></figure>
<h2 id="获取-Variable-里面的数据"><a href="#获取-Variable-里面的数据" class="headerlink" title="获取 Variable 里面的数据"></a>获取 Variable 里面的数据</h2><p>直接print(variable)只会输出 Variable 形式的数据, 在很多时候是用不了的(比如想要用 plt 画图), 所以我们要转换一下, 将它变成 tensor 形式.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">print(variable)     #  Variable 形式</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">Variable containing:</span><br><span class="line"> 1  2</span><br><span class="line"> 3  4</span><br><span class="line">[torch.FloatTensor of size 2x2]</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">print(variable.data)    # tensor 形式</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"> 1  2</span><br><span class="line"> 3  4</span><br><span class="line">[torch.FloatTensor of size 2x2]</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">print(variable.data.numpy())    # numpy 形式</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">[[ 1.  2.]</span><br><span class="line"> [ 3.  4.]]</span><br><span class="line">&quot;&quot;&quot;</span><br></pre></td></tr></table></figure></p>
<h1 id="Activation"><a href="#Activation" class="headerlink" title="Activation"></a>Activation</h1><h2 id="什么是-Activation"><a href="#什么是-Activation" class="headerlink" title="什么是 Activation"></a>什么是 Activation</h2><p>为什么需要非线性的函数？</p>
<ul>
<li>线性的话，你多少层都一样</li>
<li>非线性的话，可以拟合不同的函数</li>
</ul>
<h2 id="Torch-中的激励函数"><a href="#Torch-中的激励函数" class="headerlink" title="Torch 中的激励函数"></a>Torch 中的激励函数</h2><p>Torch 中的激励函数有很多, 不过我们平时要用到的就这几个. relu, sigmoid, tanh, softplus. 那我们就看看他们各自长什么样啦.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import torch.nn.functional as F     # 激励函数都在这</span><br><span class="line">from torch.autograd import Variable</span><br><span class="line"></span><br><span class="line"># 做一些假数据来观看图像</span><br><span class="line">x = torch.linspace(-5, 5, 200)  # x data (tensor), shape=(100, 1)</span><br><span class="line">x = Variable(x)</span><br></pre></td></tr></table></figure>
<p>接着就是做生成不同的激励函数数据:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">x_np = x.data.numpy()   # 换成 numpy array, 出图时用</span><br><span class="line"></span><br><span class="line"># 几种常用的 激励函数</span><br><span class="line">y_relu = F.relu(x).data.numpy()</span><br><span class="line">y_sigmoid = F.sigmoid(x).data.numpy()</span><br><span class="line">y_tanh = F.tanh(x).data.numpy()</span><br><span class="line">y_softplus = F.softplus(x).data.numpy()</span><br><span class="line"># y_softmax = F.softmax(x)  softmax 比较特殊, 不能直接显示, 不过他是关于概率的, 用于分类</span><br></pre></td></tr></table></figure></p>
<p>接着我们开始画图, 画图的代码也在下面:<br><img src="/images/activationfunc.jpeg" width="50%" height="50%"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt  # python 的可视化模块, 我有教程 (https://morvanzhou.github.io/tutorials/data-manipulation/plt/)</span><br><span class="line"></span><br><span class="line">plt.figure(1, figsize=(8, 6))</span><br><span class="line">plt.subplot(221)</span><br><span class="line">plt.plot(x_np, y_relu, c=&apos;red&apos;, label=&apos;relu&apos;)</span><br><span class="line">plt.ylim((-1, 5))</span><br><span class="line">plt.legend(loc=&apos;best&apos;)</span><br><span class="line"></span><br><span class="line">plt.subplot(222)</span><br><span class="line">plt.plot(x_np, y_sigmoid, c=&apos;red&apos;, label=&apos;sigmoid&apos;)</span><br><span class="line">plt.ylim((-0.2, 1.2))</span><br><span class="line">plt.legend(loc=&apos;best&apos;)</span><br><span class="line"></span><br><span class="line">plt.subplot(223)</span><br><span class="line">plt.plot(x_np, y_tanh, c=&apos;red&apos;, label=&apos;tanh&apos;)</span><br><span class="line">plt.ylim((-1.2, 1.2))</span><br><span class="line">plt.legend(loc=&apos;best&apos;)</span><br><span class="line"></span><br><span class="line">plt.subplot(224)</span><br><span class="line">plt.plot(x_np, y_softplus, c=&apos;red&apos;, label=&apos;softplus&apos;)</span><br><span class="line">plt.ylim((-0.2, 6))</span><br><span class="line">plt.legend(loc=&apos;best&apos;)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h1 id="搭神经网络"><a href="#搭神经网络" class="headerlink" title="搭神经网络"></a>搭神经网络</h1>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/17/深度学习/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chu Lin">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Truly">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/06/17/深度学习/" itemprop="url">深度学习篇</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-06-17T21:56:35-07:00">
                2018-06-17
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/deep-learning/" itemprop="url" rel="index">
                    <span itemprop="name">deep learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>虽然，强化学习、深度学习和迁移学习都是属于机器学习的，但是感觉blog里面，放在一起还是有点杂乱，所以还是分出来，慢慢整理，<br>我应该主要整理cmu <a href="https://www.cs.cmu.edu/~rsalakhu/10707/" target="_blank" rel="noopener">10707</a> introduction to deep learning的东西，也会参考<a href="http://deeplearning.cs.cmu.edu/" target="_blank" rel="noopener">11785</a> introduction to deep learning</p>
<p>就是用自己的理解整理一遍吧，之前面试总是面挂在这里，希望之后不会，还有对自己目前research也有点帮助吧。毕竟基础不牢，地动山摇。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/17/DNN/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chu Lin">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Truly">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/06/17/DNN/" itemprop="url">DNN</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-06-17T21:36:58-07:00">
                2018-06-17
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/deep-learning/" itemprop="url" rel="index">
                    <span itemprop="name">deep learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>这个是hugo的deep learning的学习<a href="http://www.dmi.usherb.ca/~larocheh/neural_networks/content.html" target="_blank" rel="noopener">tutorial</a>，需要慢慢刷掉，当然cmu 10707 的<a href="https://www.cs.cmu.edu/~rsalakhu/10707/" target="_blank" rel="noopener">slides</a>很多也是参考这个的(Russlan自己说)整理的，你如果找不到CMU 10707的视频你可以看这个，也是极好的。</p>
<p>然后，这里我想整理下最最基本的neural network的公式的推导，自己再推一遍，感觉就不虚：</p>
<h1 id="网络的基本结构："><a href="#网络的基本结构：" class="headerlink" title="网络的基本结构："></a>网络的基本结构：</h1><p><img src="/images/DNNstructure.jpeg" width="50%" height="50%"></p>
<ul>
<li>layer pre-activation for $k &gt; 0$ <strong>$(h_{0}(x) = x)$</strong><br>  $a^{(k)}(x) = b^{(k)} + W^{(k)}h^{(k - 1)}(x)$<br>  感觉这么记忆，不会记乱，k就是第几层neuron，然后$h^{(k - 1)}(x)$是前一层的输出，然后weights $W^{(k)}$，bias $b^{(k)}$都是这层的，虽然有前一层输出作为的输入，但是这些还是算这一层的。</li>
<li><p>hidden layer activation (k from 1 to L)<br>  $h^{(k)}(x) = g(a^{(k)}(x))$<br>  反正这个就是这一层的输出，然后，这层的weights，bias最终都会由这层的输出所终结。</p>
</li>
<li><p>output layer activation $k = L + 1$:<br>  $h^{(L + 1)}(x) = o(a^{(L + 1)}(x)) = f(x)$<br>  和hidden不一样就是这个是最后一层了，然后这层的activation function也会和之前hidden layers有所不同，如果是分类的话，往往是softmax</p>
</li>
<li><p>softmax activation function at the output:<br>  $o(a) = softmax(a) = [\frac{exp(a_1)}{\sum_c exp(a_c)}…\frac{a_C}{\sum_c exp(a_c)}]^T$<br>  如何理解softmax呢，为什么多分类问题里面要用softmax呢，而不是别的呢？<br>  知乎对此有一定的<a href="https://www.zhihu.com/question/40403377" target="_blank" rel="noopener">讨论</a>，我这里引用下王赟(yun第一声)大神的解答：</p>
<ul>
<li>原因之一：希望特征对概率的影响是乘性的</li>
<li>原因之二：多类分类问题的目标函数常常选为cross-entropy，…(推完整个，回来补)</li>
</ul>
</li>
<li><p>activation function:</p>
<ul>
<li><p>sigmoid：</p>
<ul>
<li>formula:<br>  $\sigma(x) = \frac{1}{1 + e^{-x}}$，$\sigma(x)’ = \frac{e^x}{(1 + e^x)^2} = (1 - \sigma(x)) \sigma(x)$</li>
<li>shortcomings:<ul>
<li>gradient vanish</li>
<li>symmetric</li>
<li>time cosuming to compute exp</li>
</ul>
</li>
</ul>
</li>
<li><p>tanh:</p>
<ul>
<li>formula:<br>  $tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} = \frac{2}{1 + e^{-2}} - 1 = 2 \sigma(2x) - 1$，$tanh(x)’ = \frac{e^x - e^{-x}}{e^x + e^{-x}}$</li>
<li>感觉就是解决了原点对称的问题</li>
</ul>
</li>
<li><p>relu:</p>
<ul>
<li>formula：$f(x) = max(0, x)$</li>
<li>优点：<br>  Relu会使一部分神经元的输出为0，这样就造成了网络的稀疏性，并且减少了参数的相互依存关系，缓解了过拟合问题的发生<br>  计算量小</li>
<li>缺点：<br>  部分neuro会死亡</li>
</ul>
</li>
<li><p>leaky relu：</p>
<ul>
<li>formula：$f(x) = max(\epsilon x, x)$</li>
<li>优点:<br>  解决了neuron会死亡的问题</li>
</ul>
</li>
<li><p>maxout：</p>
<ul>
<li>formula: 对 relu 和 leaky relu的一般归纳：$f(x) = max(w_1^T x + b_1, w_2^T x + b_2)$</li>
<li>优点:<br>  计算简单，不会死亡，不会饱和</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1 id="loss-function"><a href="#loss-function" class="headerlink" title="loss function:"></a>loss function:</h1><ul>
<li><p>stochastic gradient descent (SGD):<br>  随机梯度下降应该是最最基础的梯度下降的方法了，</p>
<ul>
<li>initialize  $\theta$ ($\theta = {W^{(1)}, b^{(1)}…，W^{(L + 1)}}$)</li>
<li>algorithm:<br>  for N iterations: (One epoch)<pre><code>for each training example $(x_{(t)}, y_{(t)})$   
    $\delta = -\nabla_{\theta}l(f(x_{(t)}, \theta), \y_{(t)}) - \lambda\nabla_{(\theta)}$
    $\omiga_{(\theta)}$
    $\theta \leftarrow \theta + \alpha \delta$
</code></pre></li>
<li>SGD 的<a href="https://zhuanlan.zhihu.com/p/22252270" target="_blank" rel="noopener">优缺点</a>：<ul>
<li>缺点：<ul>
<li>选择合适的learning rate比较困难 - 对所有的参数更新使用同样的learning rate。对于稀疏数据或者特征，有时我们可能想更新快一些对于不经常出现的特征，对于常出现的特征更新慢一些，这时候SGD就不太能满足要求了</li>
<li>相对BGD noise会比较大</li>
</ul>
</li>
</ul>
</li>
<li><p>batch gradient descent (BGD) 的<a href="https://zhuanlan.zhihu.com/p/25765735" target="_blank" rel="noopener">对比</a>：<br>  所谓batch就是一起算，你看公式就知道：<br>  $\theta \leftarrow \theta + \frac{1}{m}\sum_{i}(y_i - f(x;\theta)(x_i))$ (MSE)</p>
<ul>
<li>缺点：m很大的时候，train的会比较慢</li>
<li>优点：比SGD稳定</li>
</ul>
</li>
<li><p>mini-batch GD:<br>  就是这两个的折中，就像强化学习里面的，TD，Monta Carlo之间的n step-TD</p>
<ul>
<li>advantages:<ul>
<li>give a accurate estimate of average loss</li>
<li>can leverage matrix operations, which cost less than BGD</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li><p>what neural network estimates?<br>  $f(x)_{c} = P(y=c|x)$， where c means which class.</p>
</li>
<li><p>what to optimize?<br>  maximize log likelihood —- minize negative log likelihood: $P(y_i=c|x_i)$，given $(x_i, y_i)$<br>  cross-entropy: p, q (p one-hot, q distribution of the P(y=c|x))<br>  $l(f(x), y) = -\sum_c1(y=c)log f(x)_c = - log f(x)_y$</p>
</li>
</ul>
<h1 id="loss-gradient-output"><a href="#loss-gradient-output" class="headerlink" title="loss gradient output:"></a>loss gradient output:</h1><p><img src="/images/DNN-output-layer-gradient.jpeg" width="50%" height="50%"></p>
<h2 id="loss-gradient-at-output"><a href="#loss-gradient-at-output" class="headerlink" title="loss gradient at output"></a>loss gradient at output</h2><ul>
<li><p>partial derivative:<br>  $\frac{\partial - logf(x)_y}{\partial f(x)_c} = \frac{-1^{(y = c)}}{f(x)^y}$<br>  这里，y要和c一样才有值，因为这里cross-entropy里面用了one-hot，只有在同一维度下面，求偏导才有值。</p>
</li>
<li><p>gradient:<br>  然后，我们推广到，求梯度<br>  $\nabla_{f(x)} -logf(x)_y= \frac{-1}{f(x)_y} [1^{(y=0)}…1^{(y=C-1)}]^T = \frac{-e(y)}{f(x)^y}$</p>
</li>
</ul>
<h2 id="loss-gradient-at-output-pre-activation"><a href="#loss-gradient-at-output-pre-activation" class="headerlink" title="loss gradient at output pre-activation"></a>loss gradient at output pre-activation</h2><ul>
<li><p>partial derivative:<br>  首先还是标量的形式，<br>  $\frac{\partial - logf(x)_y}{\partial a^{(L+1)}(x)_c} = (1^{(y = c)}} - f(x)^y)$<br>  这里，y要和c一样才有值，因为这里cross-entropy里面用了one-hot，只有在同一维度下面，求偏导才有值。</p>
</li>
<li><p>gradient:<br>  然后，我们类比到向量上面，<br>  $\nabla_{a^{(L+1)}(x)_c}[- logf(x)_y}] = -(e^{(y)}} - f(x)^y)$</p>
</li>
<li><p>proof:<br>  这里的proof不完整，只是推了一个维度的，完整的可以参考ece一个师兄的知乎的<a href="https://zhuanlan.zhihu.com/p/24709748" target="_blank" rel="noopener">矩阵求导术</a>，之后回来在补推一下。<br><img src="/images/DNN-output-layer-gradient-proof.jpeg" width="70%" height="70%"></p>
</li>
</ul>
<h1 id="Backpropagation"><a href="#Backpropagation" class="headerlink" title="Backpropagation"></a>Backpropagation</h1><h2 id="Compute-output-gradient-before-activation"><a href="#Compute-output-gradient-before-activation" class="headerlink" title="Compute output gradient (before activation)"></a>Compute output gradient (before activation)</h2><p>$\nabla_{a^{(L+1)}(x)} -logf(x)_y \leftarrow - (e(y)-f(x))$</p>
<h2 id="for-k-from-L-1-to-1"><a href="#for-k-from-L-1-to-1" class="headerlink" title="for k from L + 1 to 1"></a>for k from L + 1 to 1</h2><ul>
<li>compute gradients of hidden layer parameter<br>$\nabla_{W^{(k)}} -logf(x)^y \leftarrow $ $\nabla_{a^{(k)}(x)} -log f(x)^y h^{(k-1)}(x)^T$<br>$\nabla_{b^{(k)}} -logf(x)^y \leftarrow $ $\nabla_{a^{(k)}(x)} -log f(x)^y$</li>
<li>compute gradient of hidden layer below<br>$\nabla_{b^{(k)}} -logf(x)^y \leftarrow $ $\nabla_{a^{(k)}(x)} -log f(x)^y$</li>
<li>compute gradient of hidden layer below<br>$\nabla_{h^{(k-1)}(x)} -logf(x)^y \leftarrow $ $W^{(k)T} \nabla_{a^{(k)}(x)} -log f(x)^y$</li>
<li>compute gradient of hidden layer below (before activation)<br>$\nabla_{a^{(k-1)}(x)} -logf(x)^y \leftarrow $ $(\nabla_{h^{(k-1)}(x)} -log f(x)^y) \odot […,g’(a^{(k-1)}(x)_j),…]$</li>
</ul>
<h1 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h1><ul>
<li><p>L1 &amp; L2 regularization</p>
<ul>
<li>L1 $\frac{\lambda}{2m} \sum |w|^2$<br>  L2 regularization is also known as weight decay as it forces the weights to decay towards zero (but not exactly zero).</li>
<li>L2 $\frac{\lambda}{2m} \sum |w|$<br>  Unlike L2, the weights may be reduced to zero here. Hence, it is very useful when we are trying to compress our model. Otherwise, we usually prefer L2 over it. Sparse solution.</li>
</ul>
</li>
<li><p>Dropout<br>  So what does dropout do? At every iteration, it randomly selects some nodes and removes them along with all of their incoming and outgoing connections as shown below.<br>  <img src="/images/dropout.jpeg" width="70%" height="70%"><br>  So each iteration has a different set of nodes and this results in a different set of outputs. It can also be thought of as an ensemble technique in machine learning.</p>
<p>  Ensemble models usually perform better than a single model as they capture more randomness. Similarly, dropout also performs better than a normal neural network model.</p>
<p>  This probability of choosing how many nodes should be dropped is the hyperparameter of the dropout function. As seen in the image above, dropout can be applied to both the hidden layers as well as the input layers.</p>
</li>
<li><p>Batch Normalization</p>
<ul>
<li>idea<br>  is that since it’s benefit to training if the input data is normalized, so why not normalize in hidden layers to solve the internal covariance shift.</li>
<li>denormalization<br>  to avoid extra effect of normalization, the denormalization parameters are helpful to adjust<br><img src="/images/batch-normalization.jpeg" width="70%" height="70%"></li>
</ul>
</li>
</ul>
<h1 id="Implementation-of-simple-neuron-network"><a href="#Implementation-of-simple-neuron-network" class="headerlink" title="Implementation of simple neuron network"></a>Implementation of simple neuron network</h1>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/16/周末做什么/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chu Lin">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Truly">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/06/16/周末做什么/" itemprop="url">周末做什么</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-06-16T19:43:21-07:00">
                2018-06-16
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/日常/" itemprop="url" rel="index">
                    <span itemprop="name">日常</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>很多人会烦周末了，做什么呢？<br>一些人也许会去出去和朋友浪，一些人也许会花周末的时间一直打游戏，当然也有大神会在周末继续工作。</p>
<p>我感觉把，周末最需要的是休息吧，作息规律，妥善的饮食，都是重要的。我一直觉得所谓休息不是放纵自己的欲望，一个人幸福也许是来自他/她对自己欲望的驾驭，熬夜刷剧不叫休息，熬夜看世界杯也不叫休息吧，感觉就是放纵，算是对身体的一种伤害，休息应该是不是伤害身体的。娱乐当然需要呀，晚上好好睡觉，白天起来刷剧不是更爽吗。所以，感觉休息是身体的调整吧。当然，还有精神的放松，以及卸下压力，卸下你这一周的重担。</p>
<p>当然，我感觉你觉得休息够了，你自然可以考虑工作的事情，个人发展的事情，也都挺好的，感觉这个时候更重要的是反思吧，反思这一周，所谓：学而不思则罔吧。你也可以推进一下你需要的长期的要做的事情，你可以整理一下你这一周的得失，以及对下一周的展望。</p>
<p>总之，周末就是一个节点，一个驿站，周而复始，你就会达到你所向往的未来。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/14/进阶之路/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chu Lin">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Truly">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/06/14/进阶之路/" itemprop="url">进阶之路</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-06-14T13:59:17-07:00">
                2018-06-14
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/思考/" itemprop="url" rel="index">
                    <span itemprop="name">思考</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>这篇已经看了好几遍了，但是每读每新：<br>作者：田渊栋<br>链接：<a href="https://www.zhihu.com/question/30022694/answer/224543003" target="_blank" rel="noopener">https://www.zhihu.com/question/30022694/answer/224543003</a><br>来源：知乎<br>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p>
<p>追求数目没有意义。读文章一般两个目的：</p>
<ol>
<li><p>看大家在做什么，找方向。这时候一般读读Abstract和Introduction，对领域有初步了解，知道它主要关于什么，搞清一些概念的含义和联系。这时候不懂没关系，多看几篇文章就懂了。一般聪明的人这一步可以做得很快。</p>
</li>
<li><p>搞清细节找一个自己感兴趣的方向精读，把里面的课题思路和推理细节搞明白，并且还要顺藤摸瓜找到其它大量的相关文献继续读下去。标准是在脑里能有对这个领域有清楚的脉络，能做到独立完成大部分推导和证明。一个靠谱的检查方法是给同组的人或者导师做个讲座，看他们能听懂不。很多时候自己以为懂了，其实和别人一说马上就露出马脚。同时讨论也可以激发新思路，说不定就能找到下一篇文章的出发点。这一步往往会花费一个科研人员大量时间，也是业余和职业科研的关键区别所在。总之分配给每篇文章的时间天差地别。烂文几秒钟就可以放弃，而经典文章还需要每过一阵子回头再去看一看想一想。至于如何评判文章质量，那就得要靠长年科研积累出来的品味了。接下来的两个阶段就不是光看论文可以看出来的。</p>
</li>
<li><p>写代码实现别人的工作，并且改进每篇文章都会有意或者无意抬高自己贬低别人，都存在一些有意或者无意隐藏的细节，这些不亲手做是看不到的。所以得要动手花时间去实现别人的方法，想方设法达到别人的效果，然后反过来再看看文章。时间长了马上就会学到故意隐藏的蛛丝马迹，理解别人留白的道理。光看文章的话，这类经验的积累要慢很多。一般说的“纸上谈兵”就是指这一步没做。我在15年1月刚去Facebook AI Research的时候，在深度学习上还没有实际操作经验。交给我的第一件事情是复现VGG在ImageNet上的性能，那时还没有BatchNorm，跑5个有2个能开始收敛的就不错了，最后花了几周搞定了。整个过程让我学到不少经验。</p>
</li>
<li><p>总结经验，融会贯通，找到并且遵循自己的方法论重复3很多次之后，可能会觉得自己比较有经验了。别人问起的时候也能侃侃而谈，但说的往往是一些分散且孤立的经验。并且你会发现自己很容易遗忘这些经验，这个并不是因为记忆力不好，而是因为思路不系统。这个就需要反复思考反复提炼，从而形成自己的方法论。有了方法论之后，心里就有大方向而不会随便乱试乱撞，效率就会高很多，并且能在一个科研方向上挖很深坚持很久，而不是哪个课题热做哪个。在指导别人的时候也可以做到有的放矢。在这个基础上再看文献，往往就会读懂很多一开始读不懂的东西。比如说为什么作者要强调A而否认B，那是因为他相信A后面的哲学和方法论。</p>
</li>
</ol>
<p>如果你发现自己提炼不了，或者本来知识就是凌乱的，那么要么就是(1)境界未到，要么就是(2)领域还没有成熟，目前的知识点只是零碎的拼凑。(1)要靠自己练，(2)则预示着大机遇，一个研究者牛不牛就看他是不是可以在别人都放弃的地方找到新的规律。</p>
<p>一般完成1是新闻及科普的水平，2到3是博士生低年级至高年级的水平，精通3到初入4是博后的水平，精通4则是研究员和教授的水准。另外，从1到4并没有特别固定的顺序，可能你在某个领域是4，另一个领域还只是1或2的程度；或者你在4中获得的经验能反过来帮助1和2（这个很常见）；或者一上来就可以跳过2做3，然后等3有了结果之后再去补2，等等。当然，一步跳到4那是民科的水平。</p>
<p>然后，看看我自己，应该还初入2, 3这个阶段吧，需要继续努力呀！！</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/13/批量重命名/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chu Lin">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Truly">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/06/13/批量重命名/" itemprop="url">批量重命名图片文件</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-06-13T20:51:35-07:00">
                2018-06-13
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/tech/" itemprop="url" rel="index">
                    <span itemprop="name">tech</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>转自灰羽吖大大CSDN的<a href="https://blog.csdn.net/wearge/article/details/77374150" target="_blank" rel="noopener">博客</a>：<br>其实只要对os这个包熟悉便不难，对于人脸识别项目，有些图片可能来自其他途径，这些图片常用作测试，但是对于外来图片存在命名问题。这篇就讲一下怎么实现批量重命名图片等其他文件</p>
<p>代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import os</span><br><span class="line"></span><br><span class="line">path_name=&apos;/home/huiyu/PycharmProjects/faceCodeByMe/testdata&apos;</span><br><span class="line">#path_name :表示你需要批量改的文件夹</span><br><span class="line">i=0</span><br><span class="line">for item in os.listdir(path_name):#进入到文件夹内，对每个文件进行循环遍历</span><br><span class="line">    os.rename(os.path.join(path_name,item),os.path.join(path_name,(str(i)+&apos;.jpg&apos;)))#os.path.join(path_name,item)表示找到每个文件的绝对路径并进行拼接操作</span><br><span class="line">    i+=1</span><br></pre></td></tr></table></figure>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/13/LearnAdaptSeg/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chu Lin">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Truly">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/06/13/LearnAdaptSeg/" itemprop="url">Learning to Adapt Structured Output Space for Semantic Segmentation</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-06-13T17:00:49-07:00">
                2018-06-13
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/transfer-learning/" itemprop="url" rel="index">
                    <span itemprop="name">transfer learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><a href="https://arxiv.org/abs/1802.10349" target="_blank" rel="noopener">这篇论文</a>对应的<a href="https://github.com/wasidennis/AdaptSegNet" target="_blank" rel="noopener">代码</a>是最近在跑的，需要好好看看。</p>
<h1 id="abstract"><a href="#abstract" class="headerlink" title="abstract"></a>abstract</h1><p>Convolutional neural network-based approaches for semantic segmentation rely on supervision with pixel-level ground truth, but may not generalize well to unseen image domains. As the labeling process is tedious and labor intensive, developing algorithms that can adapt source ground truth labels to the target domain is of great interest. In this paper, we propose an adversarial learning method for domain adaptation in the context of semantic segmentation. Considering semantic segmentations as structured outputs that contain spatial similarities between the source and target domains, <strong>we adopt adversarial learning in the output space</strong>. To further enhance the adapted model, we construct a <strong>multi-level adversarial network</strong> to effectively perform output space domain adaptation at different feature levels. Extensive experiments and ablation study are conducted under various domain adaptation settings, including synthetic-to-real and cross-city scenarios. We show that the proposed method performs favorably against the state-of-the-art methods in terms of accuracy and visual quality.</p>
<h1 id="introduction"><a href="#introduction" class="headerlink" title="introduction"></a>introduction</h1><h2 id="model"><a href="#model" class="headerlink" title="model"></a>model</h2><ul>
<li>1) a segmentation model to predict output results</li>
<li>2) a discriminator to distinguish whether the input is from the source or target segmentation output. <h2 id="contributions"><a href="#contributions" class="headerlink" title="contributions"></a>contributions</h2></li>
<li>propose a domain adaptation method for pixel-level semantic segmentation via adversarial learning</li>
<li>demonstrate that adaptation in the output (segmentation) space can effectively align scene layout and local context between source and target images</li>
<li>a multi-level adversarial learning scheme is developed to adapt features at different levels of the segmentation model, which leads to improved performance.</li>
</ul>
<h1 id="structure"><a href="#structure" class="headerlink" title="structure"></a>structure</h1><p>感觉也不是那么难理解，就是有点像u-net，它这里的话就是在不同的layer里面用GAN，就是所谓的multi-，<br>然后主要是output space上面，因为这篇文章发现，不管两个domain的图多么不一样，他们在output space总是有很多相似的地方。<br><img src="/images/multiGAN.jpeg" alt=""></p>
<h1 id="model-overview"><a href="#model-overview" class="headerlink" title="model overview"></a>model overview</h1><p>这个结构有两个模块：生成器$G$和判别器$D_i$ （i 表示是第几层的判别器）。images通过生成器出来的是源域segmentation的概率分布$P_s$</p>
<h1 id="loss-function"><a href="#loss-function" class="headerlink" title="loss function"></a>loss function</h1><p>$L(I_s, I_t) = L_{seg}(I_s) + \lambda L_{adv}(I_t)$</p>
<ul>
<li>$L_{seg}(I_s)$<br>  cross-entropy loss using ground truth annotations in the source domain</li>
<li>$L_{adv}$<br>  对抗损失，用来使得源域的预期的数据分布和目标域相近</li>
<li>$\lambda_{abv}$<br>  这个weight用来平衡这两个loss</li>
</ul>
<h1 id="Output-space-adaptation"><a href="#Output-space-adaptation" class="headerlink" title="Output space adaptation"></a>Output space adaptation</h1><h2 id="Single-level-adversarial-learning"><a href="#Single-level-adversarial-learning" class="headerlink" title="Single-level adversarial learning"></a>Single-level adversarial learning</h2><h3 id="Discriminator-Training"><a href="#Discriminator-Training" class="headerlink" title="Discriminator Training"></a>Discriminator Training</h3><ul>
<li>cross entropy<ul>
<li>定义：<br>  给定两个分布，p，q，它们在给定样本集上面的交叉熵的定义如下<br>  $CEH(p, q) = E_p[-log q] = - \sum_{x \in X}p(x)q(x) = H(p) + D_{KL}(p||q)$，当p的熵给定时，交叉熵和KL散度是一致是的，一定程度上可以用来描述，这两个分布的距离。</li>
<li>讨论：讲到cross entropy，为什么用cross entropy loss 于分类呢？(<a href="http://jackon.me/posts/why-use-cross-entropy-error-for-loss-function/" target="_blank" rel="noopener">Jackon解答</a>)<ul>
<li>比起一般的classification error 作为loss，它很更精细准确的去描述model的优劣</li>
<li>比起MSE，来说，它是一个凸优化的问题</li>
</ul>
</li>
</ul>
</li>
<li><p>segmentation softmax output:<br>  $P = G(I) \in R^{HxWxC}$, 这里C是种类数，这里C是2，来自源域或者来自目标域</p>
</li>
<li><p>cross-entropy loss：<br>  我们将P传到全卷积的判别器D里面：$L_d(P) = - \sum_{h, w}((1 - z)log(D(P)^{(h,w,0)})) + zlog(D(P)^{(h,w,1)})$</p>
</li>
</ul>
<h3 id="Segmentation-Network-Training"><a href="#Segmentation-Network-Training" class="headerlink" title="Segmentation Network Training"></a>Segmentation Network Training</h3><ul>
<li>segmentation loss:<br>  在源域的话我们正常训练，还是由cross-entropy loss来定义：$L_{seg}(I_s) = -\sum_{h, w}\sum_{c \in C}Y_s^{h,w,c}log(P_s^{(h,w,c)})$</li>
<li>adversarial loss：<br>  在目标域，我们的对抗损失是：$L_{adv}(I_t) = -\sum_{h,w}log(D(G(I_t)))^{(h,w,1)}$，这个损失是用来欺骗判别器的，使得两者的预期的概率的一致</li>
</ul>
<h2 id="Multi-level-Adversarial-Learning"><a href="#Multi-level-Adversarial-Learning" class="headerlink" title="Multi-level Adversarial Learning"></a>Multi-level Adversarial Learning</h2><ul>
<li>multi-level loss<br>  就是在low-level的feature space里面加上上面的loss，也不是很难理解：<br>  $L_{I_s, I_t} = \sum_i \lambda_i^{seg}(I_s) + \sum_i \lambda^i_{adv}L_{adv}^i(I_t)$，i表示第几层网络。</li>
<li>whole picture<br>  有了上面的对loss的介绍后，我们的问题其实就是一个min-max的优化问题：<br>  $max_D min_G L(I_s, I_t)$</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/8/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/8/">8</a><span class="page-number current">9</span><a class="page-number" href="/page/10/">10</a><span class="space">&hellip;</span><a class="page-number" href="/page/13/">13</a><a class="extend next" rel="next" href="/page/10/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Chu Lin</p>
              <p class="site-description motion-element" itemprop="description">去人迹罕至的地方，留下自己的足迹。</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">129</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">16</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                
                  <span class="site-state-item-count">86</span>
                  <span class="site-state-item-name">tags</span>
                
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Chu Lin</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
  


  

  

</body>
</html>
